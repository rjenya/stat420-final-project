---
title: "TBD"
output:
  html_document:
    df_print: paged
---

```{r message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(faraway)
library(knitr)
library(broom)

set.seed(20200805)
housing <- read.csv('housing.csv')
```

# Introduction

This dataset was accessed from the [Kaggle.com dataset repository](https://www.kaggle.com/austinreese/usa-housing-listings) on July 27th, 2020. It contains rental listing data scraped from the classifieds website [Craigslist.org](https://www.craigslist.org). 

After trimming, the data set includes 17 variables:

* price (Response Variable)  
* region  
* type (housing type: apartment, condo, house, etc)  
* sqft  
* beds  
* baths  
* cats_allowed  
* dogs_allowed  
* smoking_allowed  
* wheelchair_access  
* electric vehicle charge  
* comes furnished  
* laundry options  
* parking options  

The data set contains `r nrow(housing)` observations. This dataset is interesting due to its inclusion of variables representing a rental listing's "features", e.g. are pets allowed, is smoking allowed, does it come furnished, etc, as well as variables that provide context about the listing, such as geographic location and square footage. With this dataset, we can explore questions such as the following:

* Controlling for geographic location and square footage, if a renter is considering getting a pet, how much (if at all) will this increase their expected rent? 

* Controlling for geographic location and square footage, if a landlord is considering adding on-premise laundry, how much (if at all) will they be able to increase rent?

The goal of the model will be understanding how these "features" of a rental property affect price. As such, we are less concerned with optimizing the predictive performance of the model than we are with making interpretable inferences. 

# Methods

## Cleaning

To clean the prepare the data set for analysis, we will perform the following steps:

* We remove a few columns from the dataset that represent various Craigslist metadata that we will not use in analysis. These are:
    * ID
    * url
    * region_url
    * image_url
* We also remove the description data, which includes the text description of the property. This field may include interesting information, but would require parsing of the free-form text field into usable variables, and is beyond the scope of this analysis.  
* For similiar reasons, we will remove the lat/long variables  
* We cast boolean "dummy" variales as factors.

We will also filter the dataset to a single region, Jacksonville FL, and remove the region feature. We decided to perform this step after exploring initial models which included `region` as a factor variable (for which there are `r length(levels(as.factor(housing$region)))` levels). This was difficult to manage, and the models violated the equal variance assumption of linear regression. We were not able to solve this issue through transformations. 

However, we cannot ignore the effect of location. For example, if we wanted an estimate for how much on-site laundry can increase rent, it is important to remove the effect of location. Maybe listings in rural areas are more likely to have on-site laundry than listings in major cities, such that it would appear that on-site laundry is associated with lower rental price. After controlling for region, we would find that it is in fact associated with higher rental price. 

Our compromise is to remove the `region` factor, and simply focus the analysis on a single geographic region.

```{r}
housing_cleaned <- housing %>%
  filter(region == 'jacksonville',
         state == 'fl') %>%
  select(
    -c(
      id, 
      url, 
      region_url,
      image_url,
      lat,
      long,
      description)
    ) %>%
  mutate(
    type = as.factor(type),
    region = as.factor(region),
    state = as.factor(state),
    cats_allowed = as.factor(cats_allowed),
    dogs_allowed = as.factor(dogs_allowed),
    smoking_allowed = as.factor(smoking_allowed),
    wheelchair_access = as.factor(wheelchair_access),
    electric_vehicle_charge = as.factor(electric_vehicle_charge),
    comes_furnished = as.factor(comes_furnished),
    laundry_options = as.factor(laundry_options),
    parking_options = as.factor(parking_options))
```

Our cleaned dataset now has `r nrow(housing_cleaned)` rows.

## Exploratory analysis

Our dataset contains many categorical variables, which we will refer to as factors to align with the R programming language's terminology. We will begin by examining the category-level frequencies of these factors. 

```{r}
with(housing_cleaned, table(type))
with(housing_cleaned, table(cats_allowed))
with(housing_cleaned, table(dogs_allowed))
with(housing_cleaned, table(smoking_allowed))
with(housing_cleaned, table(wheelchair_access))
with(housing_cleaned, table(electric_vehicle_charge))
with(housing_cleaned, table(comes_furnished))
with(housing_cleaned, table(laundry_options))
with(housing_cleaned, table(parking_options))
```

Next, we'll examine the numeric columms, of which there are four: the predictors `sqft`, `beds` and `baths`, and the response `price`.

```{r}
ggplot(housing_cleaned, aes(x=sqfeet)) + 
  geom_density() +
  xlab('Square feet') +
  ylab('Density') +
  ggtitle('Density plot of square footage')

summary(housing_cleaned$sqfeet)
```

Although beds and baths are numeric, they have a small number of unique values, so we'll simply count up the instances of each distinct value to see if there are any outliers we need to remove.

```{r}
with(housing_cleaned, table(beds))
with(housing_cleaned, table(baths))
```

It is safe to assume that the listing with 0 beds or baths are "missing" or input errors, so we will remove those observations.


```{r}
housing_cleaned <- housing_cleaned %>%
  filter(beds > 0,
         baths > 0)

ggplot(housing_cleaned, aes(x=beds)) + geom_histogram() + ggtitle('Histogram of number of beds')
ggplot(housing_cleaned, aes(x=baths)) + geom_histogram() + ggtitle('Histogram of number of baths')
```

```{r}
ggplot(housing_cleaned, aes(x=price)) + geom_density()
summary(housing_cleaned$price)
```

The distribution of `price` is right-skewed - we'll likely explore transformations in the model fitting stage.

## Correlations

Now that we've concluded our univariate data analysis and data cleaning, we can begin exploratory analysis into correlations. This will give us an idea of what to expect from our models. Since so many of our predictors are categorical and our response is numeric, we'll make heavy use of boxplots.

```{r}
ggplot(housing_cleaned, aes(x=type, y=price)) + geom_boxplot() + ggtitle('Price v. Type')
ggplot(housing_cleaned, aes(x=cats_allowed, y=price)) + geom_boxplot() + ggtitle('Price v. Cats Allowed')
ggplot(housing_cleaned, aes(x=dogs_allowed, y=price)) + geom_boxplot() + ggtitle('Price v. Dogs Allowed')
ggplot(housing_cleaned, aes(x=smoking_allowed, y=price)) + geom_boxplot() + ggtitle('Price v. Smoking Allowed')
ggplot(housing_cleaned, aes(x=wheelchair_access, y=price)) + geom_boxplot() + ggtitle('Price v. Wheelchair Access')
ggplot(housing_cleaned, aes(x=electric_vehicle_charge, y=price)) + geom_boxplot() + ggtitle('Price v. Electric Vehicle Charge')
ggplot(housing_cleaned, aes(x=comes_furnished, y=price)) + geom_boxplot() + ggtitle('Price v. Comes Furnished')
ggplot(housing_cleaned, aes(x=laundry_options, y=price)) + geom_boxplot() + ggtitle('Price v. Laundry Options')
ggplot(housing_cleaned, aes(x=parking_options, y=price)) + geom_boxplot() + ggtitle('Price v. Parking Options')
```

```{r}
ggplot(housing_cleaned, aes(x=as.factor(beds), y=price)) + geom_boxplot() + ggtitle('Price v. Beds')
ggplot(housing_cleaned, aes(x=as.factor(baths), y=price)) + geom_boxplot() + ggtitle('Price v. Baths')
ggplot(housing_cleaned, aes(x=sqfeet, y=price)) + geom_point() + ggtitle('Square footage v. Price')
```

Judging by the bivariate correlations only (without "controlling" for any other predictors), we observe the following:

* Price generally increaes with number of beds  
* Price generally increases with number of baths    
* Price genereally increases with square footage  
* Price varies by property type, e.g. "condos" fetch higher rents than "apartments"  
* Prices appear to be higher for listings where pets are *not* allowed  
* Prices are lower on listings where smoking is allowed  
* Prices are not obviously different for listings with wheelchair access  
* Prices are considerably higher for listings with electric vechicle charge  
* Prices are higher for furnished vs. unfurnished listings  
* Price is higher when a washer/dryer is in-unit/there are in-unit hookups available

## Models - Control variables

As a reminder, our goal in this analysis is to estimate the mean change in expected rental price when a rental property has certain amenities (e.g. in-unit laundry) or allowances, such as dogs. To understand the association of an amnenity such as "in-unit laundry", it will be important to "control for" things like location, square footage, and beds/baths. 

We'll consider the following variables the "control" variables:  
* beds  
* baths  
* square footage  
* property type

Geographical location is implicitly controlled for, since our data set includes only listings in Jacksonville, FL. 

We are less interested in the specific coefficients from these variables - we simply want to make sure we are adjusting for them when analyzing our variables of interest. Since there are so many levels for the `region` factor, we will not print out those coefficeints. 

```{r}
mod_control_variables <- lm(
  price ~ 
    sqfeet +
    beds + 
    baths +
    type,
  data = housing_cleaned)

summary(mod_control_variables)$coefficients
```

These coefficient estimates are mostly intuitive and expected. The one surprising result is that the coefficients for `beds` is negative - we can see that the direction of the association between `beds` and `price` flips from postivie to negative when we add `sqfeet` to the model.

```{r}
summary(lm(price ~ beds, data=housing_cleaned))$coefficients
summary(lm(price ~ beds + sqfeet, data=housing_cleaned))$coefficients
```

This could be caused by collinearity, which we check below.

```{r}

vif(mod_control_variables)
```

Although `sqfeet` and `beds` are somewhat correlated, the VIF values of ~3 are acceptable, so the flipping of the sign does not appear to be due to collinearity. The below plot shows what is happening.

```{r}
tmp_data_for_plot <- housing_cleaned %>%
  mutate(sq_feet_ntile = ntile(sqfeet, 5)) %>%
  select(sq_feet_ntile,
         sqfeet,
         price,
         beds)

ggplot(tmp_data_for_plot, aes(x=as.factor(beds), y=price)) +
  geom_boxplot() +
  facet_wrap(~as.factor(sq_feet_ntile)) +
  xlab('Number of beds') +
  ylab('Price') +
  ggtitle('Price vs. Number of beds, facetted by square foot quintile')
```

When we facet the price vs. beds by square foot quintile, we see that the number of beds and price are in fact somewhat negatively correlated *within each quintile*. In other words, adding bedrooms while holding square footage constant decreases the price. This is inuitive, in that the size of each indidivudal bedroom becomes smaller, which devalues the property. 

## Models - variables of interest

Next we'll add our variables of interest, and confirm that the inclusion of these models is an improvement over a model with only the "control" variables. In practice, we are asking if certain features/amnenities of a rental listing helps predict price, over only the basic property facts such as square footage, number of beds, etc.

We'll compare three models: 1) the "control" model, 2) a "full" model that includes all of the variables available (control + varibles of interest), and 3) the result of applying a backwards stepwise procedure on the "full" model.

WHYUSE ADJUSTED R^2

```{r}
control_model = lm(
  log(price) ~ 
    sqfeet +
    beds + 
    baths +
    type,
  data = housing_cleaned)

full_model = lm(
  log(price) ~ 
    type + 
    sqfeet + 
    beds + 
    baths + 
    cats_allowed + 
    dogs_allowed + 
    smoking_allowed + 
    wheelchair_access + 
    electric_vehicle_charge + 
    comes_furnished + 
    laundry_options + 
    parking_options, 
  data = housing_cleaned)

selected_model = step(full_model, direction = "backward", trace=FALSE)

control_model_stats = c("Adj. R2" = summary(control_model)$adj.r.squared)
full_model_stats = c("Adj. R2" = summary(full_model)$adj.r.squared)
selected_model_stats = c("Adj. R2" = summary(selected_model)$adj.r.squared)

kable(
  data.frame("Control Model" = control_model_stats, 
             "Full Model" = full_model_stats, 
             "Selected Model" = selected_model_stats), 
  format = "markdown", 
  col.names = c("Control Model", "Full Model", "Selected Model"))
```

The adjusted $R^2$ values are about equal for the "full" and "selected" models, while the selected model uses fewer variables. So, we will move forwared with the "selected" model, and check it's assumptions. 

## Model assumptions check

We'll check for outliers first

```{r}
housing_cleaned[cooks.distance(selected_model) > 4/length(housing_cleaned),]

```

None of the observations are considered influential.

Next we'll check model's Linearity and Constant Variance assumptions


```{r}
plot(fitted(selected_model), resid(selected_model), 
       col = "darkblue", pch = 20, cex = 1.5,
       xlab = "Fitted", ylab = "Residuals")
abline(h = 0, lwd = 2)
```


```{r, message=FALSE, warning=FALSE}
library(lmtest)
bptest(selected_model)
```

Fitted vs. Residuals Plot looks reasonable but Breusch-Pagan Test rejects Constant Variance.
Even though the formal test rejects the null hypothesis (Constant Variance), we are working with a relatively large number of observation and we are satisfied that the assumptions hold given the visualizations.

Next we'll check Normality assumption

```{r}
qqnorm(resid(selected_model))
qqline(resid(selected_model))
```


```{r}
shapiro.test(resid(selected_model))
```

Q-Q plot has some tails but looks acceptable. Shapiro test, on the other hand, rejects Normality.
Like most statistical significance tests, if the sample size is sufficiently large this test may detect even trivial departures from the null hypothesis. Same as above, we trust Q-Q plot and are satisfied with Normality assumtion.

Lastly, we'll check whether we could use a better response transformation

```{r}
full_model_no_trans = lm(
  price ~ 
    type + 
    sqfeet + 
    beds + 
    baths + 
    cats_allowed + 
    dogs_allowed + 
    smoking_allowed + 
    wheelchair_access + 
    electric_vehicle_charge + 
    comes_furnished + 
    laundry_options + 
    parking_options, 
  data = housing_cleaned)
```

```{r, message=FALSE, warning=FALSE}
library(MASS)
boxcox(full_model_no_trans, plotit = TRUE)
```
```{r}
full_model_boxcox_trans = lm(
  (price^-0.6 - 1) / - 0.6 ~ 
    type + 
    sqfeet + 
    beds + 
    baths + 
    cats_allowed + 
    dogs_allowed + 
    smoking_allowed + 
    wheelchair_access + 
    electric_vehicle_charge + 
    comes_furnished + 
    laundry_options + 
    parking_options, 
  data = housing_cleaned)
selected_model_boxcox_trans = step(full_model_boxcox_trans, direction = "backward", trace=FALSE)
```
```{r}
qqnorm(resid(selected_model_boxcox_trans))
qqline(resid(selected_model_boxcox_trans))
```

We can see that boxcox transformation produced model with slightly better Normality.
We'll continue with log response transformation model because simpler transformation makes our predictor effect analysis easier.

# Results

```{r}
summary(selected_model)
```

```{r}
coef_plot_data <- tidy(selected_model) %>%
  mutate(
    ymin = estimate - (qnorm(1 - 0.05 / 2) * std.error),
    ymax = estimate + (qnorm(1 - 0.05 / 2) * std.error)) %>%
  filter(term != '(Intercept)')

ggplot(coef_plot_data, aes(x=term, y=estimate)) + 
  geom_hline(yintercept=0) +  
  geom_pointrange(aes(ymin=ymin, ymax=ymax)) +
  labs(x="Coefficient", y="Estimate", title="Coefficients from selected model") + 
  coord_flip()
```

# Discussion

We have establised that certain "features/amnenties" of a rental listing are significantly associated with price. Knowing this information helps us predict/explain price, beyond knowing the basics such as square footage, number of beds/baths, etc.

This has numerous practical implications. One is that a property owner/landlord can increase the value of their rental listing (i.e. raise rent) by improvements such as in-unit laundry, improving parking options through a garage or carport etc. They can use the coefficient estimates in a cost/benefit analysis. 

From a renters perspective, they can estimate how much additional rent they would expect to pay if requiring (for example) a furnished rental. 

The four feature/amnenity variables that are not included in the final model are `dogs_allowed`, `smoking_allowed`, `wheelchair_access`, and `electric_vehicle_charge`. Based on our earlier exploratory analysis, we suspect `dogs_allowed` was not selected due to being fairly collinear with `cats_allowed`, and `electric_vehicle_charge` was not selected due to having only a handful of observations for listings with electric vehicle charges.

### Add more discussion around the variables?
